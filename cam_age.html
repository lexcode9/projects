<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Age Estimator</title>
  <style>
    body {
      text-align: center;
    }

    #video {
      width: 90vw;
      height: auto;
      object-fit: contain;
    }
  </style>
</head>
<body>
  <video id="video" autoplay muted playsinline></video>
  <br>
  <div id="status"></div>
  <button id="switchCam">Switch Camera</button>
  <script defer src="https://cdn.jsdelivr.net/npm/face-api.js@1.2.0/dist/face-api.min.js"></script>
  <script>
    const video = document.getElementById('video');
    const statusDiv = document.getElementById('status');
    const switchCamBtn = document.getElementById('switchCam');
    let speaking = false;
    let useFrontCam = true;
    let currentStream;
    let detectionInterval;

    async function loadModels() {
      await faceapi.nets.tinyFaceDetector.loadFromUri('https://cdn.jsdelivr.net/npm/face-api.js@1.2.0/models');
      await faceapi.nets.ageGenderNet.loadFromUri('https://cdn.jsdelivr.net/npm/face-api.js@1.2.0/models');
    }

    function speak(text) {
      if (speaking) return;
      const msg = new SpeechSynthesisUtterance(text);
      msg.lang = 'en-US';
      msg.rate = 1;
      msg.pitch = 0.9;
      msg.onstart = () => speaking = true;
      msg.onend = () => { speaking = false; };
      window.speechSynthesis.speak(msg);
    }

    async function startCamera() {
      if (currentStream) {
        currentStream.getTracks().forEach(track => track.stop());
      }
      clearInterval(detectionInterval);

      const constraints = {
        video: {
          facingMode: useFrontCam ? 'user' : 'environment'
        }
      };

      try {
        const stream = await navigator.mediaDevices.getUserMedia(constraints);
        currentStream = stream;
        video.srcObject = stream;
        video.style.transform = useFrontCam ? 'scaleX(-1)' : 'none';
        video.onloadeddata = () => {
          runDetection();
        };
      } catch (e) {
        statusDiv.textContent = "Error: Camera access denied.";
      }
    }

    async function runDetection() {
      statusDiv.textContent = "Status: Loading models...";
      await loadModels();
      statusDiv.textContent = "Status: Scanning...";

      detectionInterval = setInterval(async () => {
        const detections = await faceapi.detectSingleFace(video, new faceapi.TinyFaceDetectorOptions()).withAgeAndGender();
        if (detections) {
          const age = Math.round(detections.age);
          statusDiv.textContent = `Status: ${age} years old`;
          speak(age + ' years old');
        } else {
          statusDiv.textContent = "Status: Scanning...";
        }
      }, 1000);
    }

    switchCamBtn.addEventListener('click', async () => {
      useFrontCam = !useFrontCam;
      await startCamera();
    });

    startCamera();
  </script>
</body>
</html>
