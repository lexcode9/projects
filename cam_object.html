<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Cam Object Detector</title>
 <style>
#video {
  transform: scaleX(-1);
}

@media (orientation: portrait) {
  #video {
    width: 100vw;
    height: 100vh;
  }
}

@media (orientation: landscape) {
  #video {
    width: 100vh;
    height: 100vw;
  }
}
</style>
</head>
<body>
  <center>
    <video id="video" autoplay muted playsinline></video><br>
    <button id="switchCam">Switch Camera</button>
  </center>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/coco-ssd"></script>

  <script>
    const video = document.getElementById('video');
    const statusDiv = document.getElementById('status');
    const switchCamBtn = document.getElementById('switchCam');

    let lastSpoken = '';
    let speaking = false;
    let useFrontCam = true;
    let currentStream;

    function speak(text) {
      if (speaking || text === lastSpoken) return;
      const msg = new SpeechSynthesisUtterance(text);
      msg.lang = 'en-US';
      msg.rate = 1;
      msg.pitch = 0.9;
      msg.onstart = () => speaking = true;
      msg.onend = () => { speaking = false; lastSpoken = text; };
      window.speechSynthesis.speak(msg);
    }

    async function startCamera() {
      if (currentStream) {
        currentStream.getTracks().forEach(track => track.stop());
      }

const constraints = {
  video: {
    facingMode: useFrontCam ? 'user' : 'environment'
  }
};

      try {
        const stream = await navigator.mediaDevices.getUserMedia(constraints);
        currentStream = stream;
        video.srcObject = stream;

        video.style.transform = useFrontCam ? 'scaleX(-1)' : 'none';

      } catch (e) {
        alert("Could not access camera: " + e);
      }
    }

    async function runDetection() {
      const model = await cocoSsd.load();
      setInterval(async () => {
        statusDiv.textContent = "scanning";
        const predictions = await model.detect(video);
        if (predictions.length > 0) {
          const top = predictions[0];
          statusDiv.textContent = top.class;
          speak(top.class);
        }
      }, 2000);
    }

    switchCamBtn.addEventListener('click', async () => {
      useFrontCam = !useFrontCam;
      await startCamera();
    });

    startCamera().then(runDetection);
  </script>
</body>
</html>






